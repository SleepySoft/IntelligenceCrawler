# CrawlerGenerated.py - This code is generated by CrawlerPlayground

from IntelligenceCrawler.CrawlPipeline import *

def run_pipeline():
    # === 1. Initialize Components ===
    d_fetcher = RequestsFetcher(log_callback=log_cb, proxy=None, timeout_s=10)
    e_fetcher = RequestsFetcher(log_callback=log_cb, proxy=None, timeout_s=20)
    discoverer = SitemapDiscoverer(fetcher=d_fetcher, verbose=True)
    extractor = Crawl4AIExtractor(verbose=True)
    
    # === 2. Define Parameters ===
    entry_point = 'http://www.people.com.cn/'
    start_date = None
    end_date = None
    extractor_kwargs = {}
    channel_filter_list = [
        'ah/news_sitemap.xml',
        'ent/news_sitemap.xml',
        'gz/news_sitemap.xml',
        'nx/news_sitemap.xml',
        'sn/news_sitemap.xml',
        'society/news_sitemap.xml',
        'theory/news_sitemap.xml',
        'xj/news_sitemap.xml',
    ]

    # === 3. Build pipeline ===
    pipeline = CrawlPipeline(
        d_fetcher=d_fetcher,
        discoverer=discoverer,
        e_fetcher=e_fetcher,
        extractor=extractor,
        log_callback=log_cb
    )

    # Step 1: Discover all channels
    pipeline.discover_channels(entry_point, start_date, end_date)

    # Step 2: Discover and fetch articles (populates pipeline.contents)
    pipeline.discover_articles(channel_filter=partial(
        common_channel_filter, channel_filter_list=channel_filter_list))

    # Step 3: Extract content and run handlers
    pipeline.extract_articles(
        article_filter=lambda url: True,
        content_handler=save_article_to_disk,
        exception_handler=lambda url, exception: None,
        **extractor_kwargs
    )


if __name__ == "__main__":
    try:
        run_pipeline()
    except Exception as e:
        print(str(e))
        print(traceback.format_exc())
