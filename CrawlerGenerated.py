# CrawlerGenerated.py - This code is generated by CrawlerPlayground

from IntelligenceCrawler.CrawlPipeline import *

def run_pipeline():
    # === 1. Initialize Components ===
    d_fetcher = PlaywrightFetcher(log_callback=log_cb, proxy=None, timeout_s=10, stealth=True, pause_browser=False, render_page=False)
    e_fetcher = PlaywrightFetcher(log_callback=log_cb, proxy=None, timeout_s=20, stealth=True, pause_browser=False, render_page=True)
    discoverer = ListPageDiscoverer(fetcher=d_fetcher, verbose=True, manual_specified_signature='div.fr > div.clearfix.pNcon > ul.clearfix.listN > li > a')
    extractor = TrafilaturaExtractor(verbose=True)
    
    # === 2. Define Parameters ===
    entry_point = 'https://world.people.com.cn/GB/1030/index.html'
    start_date = None
    end_date = None
    extractor_kwargs = {'wait_until': 'networkidle', 'wait_for_selector': None, 'wait_for_timeout_s': 20}
    channel_filter_list = []

    # === 3. Build pipeline ===
    pipeline = CrawlPipeline(
        d_fetcher=d_fetcher,
        discoverer=discoverer,
        e_fetcher=e_fetcher,
        extractor=extractor,
        log_callback=log_cb
    )

    # Step 1: Discover all channels
    pipeline.discover_channels(entry_point, start_date, end_date)

    # Step 2: Discover and fetch articles (populates pipeline.contents)
    pipeline.discover_articles(channel_filter=partial(
        common_channel_filter, channel_filter_list=channel_filter_list))

    # Step 3: Extract content and run handlers
    pipeline.extract_articles(
        article_filter=lambda url: True,
        content_handler=save_article_to_disk,
        exception_handler=lambda url, exception: None,
        **extractor_kwargs
    )


if __name__ == "__main__":
    try:
        run_pipeline()
    except Exception as e:
        print(str(e))
        print(traceback.format_exc())
